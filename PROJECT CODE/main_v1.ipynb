{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('miniconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "dc9d0d03c2bf962b4475c46075ef9b99c570b548779db567b1b1c538bd29eb37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import processing\n",
    "from models import LogisticRegression\n",
    "from models import RandomForest, SVM, KNN\n",
    "from models import evaluate_model # grid_search, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "from fomlads.model.classification import logistic_regression_fit\n",
    "from fomlads.model.classification import logistic_regression_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from processing import accuracy_score, train_test_split\n",
    "from evaluation import classificationreport\n",
    "from evaluation import confusion_matrix\n",
    "#from evaluation import plot_cm\n",
    "from time import process_time\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, lamda, add_bias_term):\n",
    "        self.lamda = lamda\n",
    "        self.add_bias_term = add_bias_term\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.weights = logistic_regression_fit(X_train, y_train, lamda=self.lamda)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_predict = logistic_regression_predict(X_test, self.weights)\n",
    "        return y_predict\n",
    "\n",
    "\n",
    "\n",
    "RandomForest = RandomForestClassifier\n",
    "SVM = lambda C, gamma: SVC(C=C, gamma=gamma, kernel='rbf')\n",
    "KNN = KNeighborsClassifier\n",
    "\n",
    "def cross_validation(model, X, y, cv=5):\n",
    "    if cv == 1:\n",
    "        X_train, y_train, X_test, y_test = train_test_split(X, y, test_frac=0.2, state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        val_acc = accuracy_score(y_test, y_predict)\n",
    "        return [val_acc]\n",
    "\n",
    "    Xfolds = np.array_split(X, cv)\n",
    "    yfolds = np.array_split(y, cv)\n",
    "    cross_vals = []\n",
    "    for fold in range(cv):\n",
    "        y_test = yfolds[fold]\n",
    "        X_test = Xfolds[fold]\n",
    "        \n",
    "        X_train_temp = Xfolds[:fold]\n",
    "        X_train_temp.extend(Xfolds[fold+1:])\n",
    "        X_train = np.vstack(X_train_temp)\n",
    "        y_train_temp = yfolds[:fold]\n",
    "        y_train_temp.extend(yfolds[fold+1:])\n",
    "        y_train = np.hstack(y_train_temp)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        val_acc = accuracy_score(y_test, y_predict)\n",
    "        cross_vals.append(val_acc)\n",
    "    return cross_vals\n",
    "\n",
    "def evaluate_model(name, X_train, y_train, X_test, y_test, hyper,group):\n",
    "    m2m = {'SVM': (SVM), 'RF': (RandomForest), 'KNN': (KNeighborsClassifier), 'Logistic': (LogisticRegression)}\n",
    "    Model = m2m[name]\n",
    "    model = Model(**hyper)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "\n",
    "    # Performance report\n",
    "    plot_cm(y_test, y_predict,group)\n",
    "    classificationreport(y_test, y_predict)\n",
    "\n",
    "\n",
    "\n",
    "# Function for cross validation on logistic regression\n",
    "def cross_validation_LR(X, y, cv,lamda=0):\n",
    "    \"\"\"\n",
    "    X = training_validation_inputs\n",
    "    y = training_validation_targets\n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Xfolds = np.array_split(X, cv)\n",
    "    yfolds = np.array_split(y, cv)\n",
    "    cross_vals = []\n",
    "    for fold in range(cv):\n",
    "        y_test = yfolds[fold]\n",
    "        X_test = Xfolds[fold]\n",
    "        \n",
    "        X_train_temp = Xfolds[:fold]\n",
    "        X_train_temp.extend(Xfolds[fold+1:])\n",
    "        X_train = np.vstack(X_train_temp)\n",
    "        y_train_temp = yfolds[:fold]\n",
    "        y_train_temp.extend(yfolds[fold+1:])\n",
    "        y_train = np.hstack(y_train_temp)\n",
    "        weights = logistic_regression_fit(X_train, y_train,lamda=lamda)\n",
    "        y_predict = logistic_regression_predict(X_test,weights)\n",
    "        val_acc = accuracy_score(y_test,y_predict)\n",
    "        cross_vals.append(val_acc)\n",
    "    return cross_vals\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def evaluate_cv_LR(X, y,lamda=0, output=False):\n",
    "        \n",
    "    scores = cross_validation_LR(X, y, cv=5,lamda=lamda)\n",
    "    avg_cv = np.mean(scores)\n",
    "    var_cv = np.std(scores)\n",
    "    if output:\n",
    "        print(model)\n",
    "        print(\"avg cv score is:\", avg_cv, \"std is:\", var_cv)\n",
    "    return avg_cv, var_cv\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def LR_lambda_cv(training_validation_inputs,training_validation_targets,test_inputs,test_targets,wine_type,lambda_list):\n",
    "    \"\"\"\n",
    "    This function finds the best parameter for lambda on logistic regression then runs the found parameters\n",
    "    on the test data. It takes in a list of lambdas and tests the values provided within the range. \n",
    "    It plots a graph for both red and white wine showing the best value of lambda for both partitions. It then runs the model with the\n",
    "    best parameters on the test data and outputs the precision, recall  f1 score , and accuracy as well as timing the process.\n",
    "    \n",
    "    \"\"\"\n",
    "    accuracy_list = []\n",
    "    print(f\"Testing lambda parameter on validation data for {wine_type} consisting of 5 folds\")\n",
    "    for lam in lambda_list:\n",
    "\n",
    "        val_acc,var_cv=evaluate_cv_LR(training_validation_inputs,training_validation_targets,lamda=lam)\n",
    "        accuracy_list.append(val_acc)\n",
    "    \n",
    "    score_max = max(accuracy_list)\n",
    "    lam_max = lambda_list[accuracy_list.index(score_max)]\n",
    "    \n",
    "    plt.plot(lambda_list, accuracy_list, label = f'{wine_type}')\n",
    "    \n",
    "    plt.plot(lam_max, score_max, 'o', color = 'orange')\n",
    "    \n",
    "    plt.xlabel('Lambda')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Average Logistic regression accuracies on values of lambda on validation data\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Lambda_fitting_on_cross_validation_logistic_regression.png', bbox_inches='tight')\n",
    "    \n",
    "    print(f'\\nBest parameter(lambda) for logistic regression for {wine_type} on validation data is ' + str(lam_max))\n",
    "    print(f'\\nBest average accuracy score for logistic regression for {wine_type} on validation data is ' + str(score_max))\n",
    "    print('\\nNow running logistic regression on test data with best parameters ...')\n",
    "    \n",
    "    # LR_test_funct(training_validation_inputs,training_validation_targets,test_inputs,test_targets,wine_type=wine_type,lamda = lam_max)\n",
    "    \n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def LR_test_funct(train_val_inputs,train_val_targets,test_inputs,test_targets, wine_type,lamda = 0):\n",
    "    \n",
    "    t1_start = process_time()\n",
    "\n",
    "    weight = logistic_regression_fit(train_val_inputs,train_val_targets,lamda = lamda)\n",
    "    predicted_wine_targets = logistic_regression_predict(test_inputs,weight)\n",
    "    \n",
    "    t1_stop = process_time()\n",
    "    \n",
    "    print(\"Time taken for model to run on test data in seconds: \",t1_stop-t1_start)\n",
    "    print(f\"\\n Classification Report for {wine_type} on test data \\n\\n\")\n",
    "    classificationreport(test_targets, predicted_wine_targets)\n",
    "    print(f'\\n Confusion matrix for {wine_type} on test data \\n\\n' + str(confusion_matrix(test_targets,predicted_wine_targets)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier\n",
    "\n",
    "def grid_search(name, X, y, cv=5, N=5):\n",
    "    \n",
    "    svm_hyper = {'C': np.logspace(-4, 1, N), 'gamma': np.logspace(-1, 2, N)}\n",
    "    rf_hyper = {'n_estimators': [10, 100, 1000], 'max_depth': np.arange(1, 11)}\n",
    "    #logist_hyper = {'lr' : [0.001, 0.01, 0.1], 'regularization': ['none',]}\n",
    "    logist_hyper = {'lamda' : np.logspace(-4, -1, N), 'add_bias_term': [True]}\n",
    "    knn_hyper = {'n_neighbors' : np.arange(1, 11), 'weights': ['uniform', 'distance']}\n",
    "\n",
    "    #m2m = {'SVM': (SVM, svm_hyper), 'RF': (RandomForest, rf_hyper), 'Logistic': (LogisticRegression, logist_hyper)}\n",
    "    m2m = {'SVM': (SVM, svm_hyper), 'RF': (RandomForest, rf_hyper),  'KNN': (KNeighborsClassifier, knn_hyper),'Logistic': (LogisticRegression, logist_hyper)}\n",
    "    Model, params = m2m[name]\n",
    "\n",
    "    akey, bkey = params.keys()\n",
    "    alphas = params[akey]\n",
    "    betas = params[bkey]        \n",
    "    scores = [] # scores with hyperparameter\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            hyper = dict()\n",
    "            hyper[akey] = alpha\n",
    "            hyper[bkey] = beta\n",
    "            model = Model(**hyper)\n",
    "            avg_score = np.mean(cross_validation(model, X, y, cv=cv))\n",
    "            print(avg_score,hyper)\n",
    "            scores.append((avg_score, hyper))\n",
    "    \n",
    "    #print(type(scores))\n",
    "    sns.heatmap(np.array(scores))\n",
    "\n",
    "    # x = np.array((scores))\n",
    "    # x_res=x.reshape(3,4)\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(15,15))\n",
    "    # sns.heatmap(x_res, square=True, ax=ax)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "            \n",
    "    return max(scores, key=lambda ele : ele[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exploring and Processing dataset\n",
      "[Red wine group]\n",
      "Deriving the representation...\n",
      "feature mapping...\n",
      "standardizing...\n",
      "splitting the dataset...\n",
      "[White wine group]\n",
      "Deriving the representation...\n",
      "feature mapping...\n",
      "standardizing...\n",
      "splitting the dataset...\n",
      "Performing grid search\n",
      "Best Parameter for red wine:\n",
      "0.7011564211807669 {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "0.7011564211807669 {'n_neighbors': 1, 'weights': 'distance'}\n",
      "0.6713329275715155 {'n_neighbors': 2, 'weights': 'uniform'}\n",
      "0.7011564211807669 {'n_neighbors': 2, 'weights': 'distance'}\n",
      "0.7334144856968959 {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.7328058429701765 {'n_neighbors': 3, 'weights': 'distance'}\n",
      "0.7346317711503347 {'n_neighbors': 4, 'weights': 'uniform'}\n",
      "0.7370663420572124 {'n_neighbors': 4, 'weights': 'distance'}\n",
      "0.7376749847839318 {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "0.7358490566037735 {'n_neighbors': 5, 'weights': 'distance'}\n",
      "0.7492391965916008 {'n_neighbors': 6, 'weights': 'uniform'}\n",
      "0.7382836275106512 {'n_neighbors': 6, 'weights': 'distance'}\n",
      "0.7468046256847231 {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "0.7449786975045648 {'n_neighbors': 7, 'weights': 'distance'}\n",
      "0.7492391965916008 {'n_neighbors': 8, 'weights': 'uniform'}\n",
      "0.7474132684114425 {'n_neighbors': 8, 'weights': 'distance'}\n",
      "0.7534996956786366 {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "0.7547169811320755 {'n_neighbors': 9, 'weights': 'distance'}\n",
      "0.7534996956786366 {'n_neighbors': 10, 'weights': 'uniform'}\n",
      "0.7522824102251978 {'n_neighbors': 10, 'weights': 'distance'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hq/mwjvhf0x5vd7q68wfyy9mnh00000gn/T/ipykernel_63731/350664488.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing grid search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Parameter for red wine:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mr_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Parameter for white wine:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hq/mwjvhf0x5vd7q68wfyy9mnh00000gn/T/ipykernel_63731/3104478884.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(name, X, y, cv, N)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#print(type(scores))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# x = np.array((scores))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \"\"\"\n\u001b[1;32m    539\u001b[0m     \u001b[0;31m# Initialize the plotter object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[0m\u001b[1;32m    541\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                           yticklabels, mask)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# Determine good default values for the colormapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         self._determine_cmap_params(plot_data, vmin, vmax,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                     cmap, center, robust)\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_determine_cmap_params\u001b[0;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# plot_data is a np.ma.array instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcalc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "    print(\"Exploring and Processing dataset\")\n",
    "    \n",
    "    state = 42\n",
    "    print(\"[Red wine group]\")\n",
    "    rX_train, ry_train, rX_test, ry_test = processing(\"winequalityN.csv\", 100, group = \"red\", test_frac=0.2, state=state)\n",
    "  \n",
    "    print(\"[White wine group]\")\n",
    "    wX_train, wy_train, wX_test, wy_test = processing(\"winequalityN.csv\", 100, group = \"white\", test_frac=0.2, state=state)\n",
    "    \n",
    "\n",
    "    print(\"Performing grid search\")\n",
    "    print(\"Best Parameter for red wine:\")\n",
    "    r_val_acc, r_hyper = grid_search('KNN', rX_train, ry_train, cv=1, N=3)\n",
    "    print(r_val_acc, r_hyper)\n",
    "    print(\"Best Parameter for white wine:\")\n",
    "    w_val_acc, w_hyper = grid_search('KNN', wX_train, wy_train, cv=1, N=3)\n",
    "    print(w_val_acc, w_hyper)\n",
    "\n",
    "    # print(\"Evaluate the model using best hyperparameters found\")\n",
    "    # evaluate_model('KNN', rX_train, ry_train, rX_test, ry_test, r_hyper)\n",
    "    # evaluate_model('KNN', wX_train, wy_train, wX_test, wy_test, w_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}